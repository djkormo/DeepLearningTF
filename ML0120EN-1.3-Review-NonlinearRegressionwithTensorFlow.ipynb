{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "<h1 align=\"center\"> Non Linear Regression </h1><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>In this section we will overview the Non Linear Regression and using TensorFlow for Non Linear Regression</strong></font>\n",
    "<br>\n",
    "- <p><a href=\"#ref1\">Non Linear Regression</a></p>\n",
    "- <p><a href=\"#ref1.1\">Link function </a></p>\n",
    "    * <a href=\"#ref1.2\">sigmoid</a>\n",
    "    * <a href=\"#ref1.3\">Log</a>\n",
    "- <p><a href=\"#ref2\">Non Linear Regression with TensorFlow</a></p>\n",
    "    * <a id=\"#ref2.1\"> Explore the Data</a>\n",
    "    * <a href=\"#ref2.3\">Model setup in TensorFlow</a>\n",
    "    * <a href=\"#ref2.4\">Loss function setup  in TensorFlow</a>\n",
    "    * <a href=\"#ref2.5\">Training the model</a>\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "## Non Linear Regression\n",
    "<h6>Let's learn about non linear regressions and apply an example on TensorFlow</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Non-linear regressions are a relationship between independent variables $x$ and a dependent variable $y$ which result in a non-linear function modeled data. Essentially any relationship that is not linear can be termed as non-linear, and is usually represented by the polynomial of $k$ degrees (maximum power of $x$). This expression can be further transformed on the right side of the equation with function which are called as **link** functions denoted by $f$. some of the well-known link functions are sigmoid ( also known as logit in statistics world), log, log-complmentary-log, and exponential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "y = f(\\beta_0+ \\beta_1 x + \\beta_2 x^2 +...+\\beta_k x^k) \\dots \\ (1)\n",
    "\\end{equation}\n",
    "where, $\\beta_i$'s are parameters to be estimated that makes the model fit perfectly to the underlying data.\n",
    "Let us consider one of the simple scenario where the degrees of the non-linear polynomial regression is 2(quadratic) and 3(cubic), and the link function is **identity**, so the equation is written simply as: \n",
    "$$y = \\beta_0+ \\beta_1 x + \\beta_2 x^2 +...+\\beta_k x^k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "\n",
    "## Please feel free to change the parameters\n",
    "beta0=1\n",
    "beta1=1\n",
    "beta2=1   # this represents quadratic term\n",
    "beta3=0  # this represents cubic term\n",
    "\n",
    "y= beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)\n",
    "\n",
    "plt.plot(x,y) \n",
    "plt.ylabel('Dependent Variable')\n",
    "plt.xlabel('Indepdendent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this function has $x^3$ and $x^2$ as independent variables. Also, the graphic of this function is not a straight line over the 2D plane. So this is a non-linear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1.1\"></a>\n",
    "### Link function: \n",
    "The function $f$ represented in equation $(1)$ is a link function or also refered to as activation function in machine-learning world. Note that the activation function always get linear input whereas here we can give non-linear inputs to the function: There few highly used such link functions like sigmoid, log, complementary log-log, Gaussian, or Identity function; any other form of such function would be sub-case of these functions or an apparent of inverse of the these functions. \n",
    "\n",
    "<a id=\"ref1.2\"></a>\n",
    "#### sigmoid\n",
    "Sigmoid is a transformation and it is expressed as follows\n",
    "\\begin{equation}\n",
    "y = \\frac{1}{1+e^{-X}}  \\dots \\ (2)\n",
    "\\end{equation}\n",
    "whereas, $X$ is $ (\\beta_0+ \\beta_1 x + \\beta_2 x^2 +...+\\beta_k x^k) $. This link function restricts the response between values 0 to 1.  There are many different forms of these sigmoid link functions which would map the inputs (a.k.a independent variables: $x$) to output (a.k.a dependent variable $y$) between -1 to 1, which is perferred in machine learning world.\n",
    "\n",
    "For now we would be working with simplest case of this sigmoid function on the one degree polynomial function denoted in the equation $(2)$ as : $ y = \\frac{1}{1+e^{-x}} $. ** Make sure to work with different degrees of polynomial function with in it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "\n",
    "## Please feel free to change the parameters\n",
    "beta0=0\n",
    "beta1=1\n",
    "beta2=0   # this represents quadratic term\n",
    "beta3=0  # this represents cubic term\n",
    "\n",
    "\n",
    "X =  beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)\n",
    "\n",
    "# linking function.\n",
    "y = 1/(1+np.exp(-X))\n",
    "\n",
    "plt.plot(x,y) \n",
    "plt.ylabel('Dependent Variable')\n",
    "plt.xlabel('Indepdendent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1.3\"></a>\n",
    "#### Log\n",
    "The response $y$ is a results of applying logarithmic map from input $x$'s to output variable $y$, in general form it would be written as  \n",
    "\\begin{equation}\n",
    "y = \\log(X)\n",
    "\\end{equation}\n",
    "\n",
    "conditional on  $X$ being positive, whereas, as described in previous section $X$ is polynomial representation of the $x$'s. One of the simplest form of the above relationship would be with polynomial with 1 degree, and no parameters to be estimated i.e. $$ y = \\log(x)$$\n",
    "\n",
    "**Note: try out few beta parameters with non-zero-values to see the effect on curve **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.1, 5.0, 0.1)\n",
    "\n",
    "## Please feel free to change the parameters\n",
    "beta0=0\n",
    "beta1=1\n",
    "beta2=0   # this represents quadratic term\n",
    "beta3=0  # this represents cubic term\n",
    "\n",
    "X = beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)\n",
    "\n",
    "# mapping via link function\n",
    "y = np.log(X)\n",
    "\n",
    "plt.plot(x,y) \n",
    "plt.ylabel('Dependent Variable')\n",
    "plt.xlabel('Indepdendent Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2\"></a>\n",
    "# Non-Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we would be looking at the use of tensorflow paradigm by building the session and then training the model using data. The process of training involves optimizing the loss functions  using techniques such as gradient descent approach and find the best possible solutions for the parameters of interest. \n",
    "\n",
    "    \n",
    "The following data is from Fernandez-Juricic, et al. (2003). The study examined the effect of human disturbance on the nesting of house sparrows (Passer domesticus). Breeding pairs of birds (\"pairs\") per hectare were counted in 23 parks in Madrid, Spain. They also counted the number of people walking through the park per hectare per minute (\"pedestrians\"). The relationship is nonlinear and nonmonotonic, as shown by a scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# #downloading dataset\n",
    "## !wget -nv -O /resources/data/china_gdp.csv https://ibm.box.com/shared/static/ccd2tu4wvkwi1f6yp4mm1ztsyt1ygphv.csv\n",
    "# df = pd.read_csv(\"/resources/data/china_gdp.csv\")\n",
    "\n",
    "# JAG this is to be switche with DSWB path\n",
    "df = pd.read_csv(\"https://ibm.box.com/shared/static/7tr6tai74kdk3vik815k2frl54kfuw4h.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2.1\"></a>\n",
    "### Explore the Data\n",
    "The data corresponds to determination of the age by the length of the fish, each data point is the measurement on a fish. Its kind of looks like an exponential function. The growth starts off slow, then from 2005 on forward, the growth is very significant. And finally, it deaccelerates slightly in the 2010s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = (df[\"pedestrian\"].values, df[\"pairs\"].values)\n",
    "\n",
    "# plots the data points\n",
    "plt.plot(x_data, y_data, 'ro')\n",
    "\n",
    "# label the axis\n",
    "plt.xlabel(\"# of walkers in park\")\n",
    "plt.ylabel(\"# breeding pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an initial look at the plot, we determine that the three degree non-linear regression function could be a good approximation. As the data shows more than 2 curvatures, we can start 3 degree polynomial regression\n",
    "let us fit the data with an arbitrary 3rd degree polynomial model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please feel free to change the parameters\n",
    "x = x_data\n",
    "beta0=250    # represents the intercept on yaxis\n",
    "beta1=-10.0   # represents the slope of the line\n",
    "beta2= 0   # this represents quadratic term\n",
    "beta3=-0.0  # this represents cubic term\n",
    "\n",
    "\n",
    "y =  beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)\n",
    "\n",
    "plt.plot(x_data,y_data,'ro')\n",
    "plt.plot(x_data,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2.3\"></a>\n",
    "### Model setup in TensorFlow\n",
    "Now, let's build our regression model and initialize its parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input for the model, i.e, the 'X' variable\n",
    "# note that placeholder implies that we have pass this parameter when we invoke the \n",
    "# tensorflow paradigm\n",
    "x = tf.placeholder(tf.float32, shape=(x_data.size))\n",
    "y = tf.placeholder(tf.float32,shape=(y_data.size))\n",
    "\n",
    "# tf.Variable call creates a  single updatable copy in the memory and efficiently updates \n",
    "# the copy to relfect any changes in the variable values through out the scope of the tensorflow session\n",
    "beta_0 = tf.Variable(100.0)\n",
    "beta_1 = tf.Variable(0.0)\n",
    "beta_2 = tf.Variable(0.0)\n",
    "beta_3 = tf.Variable(0.0)\n",
    "\n",
    "\n",
    "y_pred = tf.add(  beta_0,\n",
    "                    tf.add( tf.multiply(beta_1,x), \n",
    "                           tf.add( tf.multiply(beta_2, tf.multiply(x,x)),\n",
    "                                  tf.multiply(beta_3, tf.multiply(x,tf.multiply(x,x))))))\n",
    "\n",
    "\n",
    "#create session and initialize variables\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "#get prediction with initial parameter values\n",
    "pred = session.run(y_pred, feed_dict={x:x_data, y:y_data})\n",
    "\n",
    "#plot initial prediction against datapoints\n",
    "plt.plot(x_data, pred)\n",
    "plt.plot(x_data, y_data, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, as you can see that the our model setup using TensorFlow operator is successful and the plots produced in same as that we saw in the earlier section. Now, we are going to define a loss function for our regression, so we can train our model to better fit our data. We use mean squared error as our loss function, there are multiple different loss criterias that can be optimized which are based on the distance metric between predicted and the true values of the $Y$\n",
    "\n",
    "$$ loss = \\frac{1}{n}\\sum_{i=1}^n{[Y_i - \\hat{Y}_i]^2} $$\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"ref2.4\"></a>\n",
    "### Loss function setup  in TensorFlow\n",
    "\n",
    "Let us setup the loss function based on the TensorFlow functions, and then have a trial run of the session to see if the loss is calculated without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization factor\n",
    "nf = 1e-1\n",
    "# loss = tf.reduce_mean(tf.squared_difference(Y_pred * nf, Y * nf))\n",
    "# seting up the loss function\n",
    "loss = tf.reduce_mean(tf.squared_difference(y_pred*nf,y*nf))\n",
    "\n",
    "# check you loss function setup\n",
    "session.run(loss, feed_dict={x:x_data,y:y_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we bind our loss function to a Gradient Descent Optimizer and *test the learning-rate by re-running the box below  2 to 3 times*. If the loss starts to increase then its good idea to reduce your learning-rate or by reducing the normalization fractor in the above step. Note these gradient optimizations when its comes to non-linear model can be pretty unstable depending on how what is the step-size in the optimization. The gradient Descent optimizer takes in parameter: learning rate, which corresponds to the speed with which the optimizer should learn; there are pros and cons for increasing the learning-rate parameter, training model converges quickly, whereas there is risk of optimizer getting stuck into local optimum solutions **Please feel free to make changes to learning parameter and check its effect**. On the other hand decreasing the learning rate might reduce the convergence speed, but would improve on the optimal solution by not getting stuck in local optimal solutions. Please review other material for further information on the optimization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.000021)\n",
    "optimizer = tf.train.AdagradOptimizer(0.01 )\n",
    "\n",
    "# pass the loss function that optimizer should optimize on.\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_i, loss_i= session.run([train,loss], feed_dict={x:x_data,y:y_data})\n",
    "\n",
    "\n",
    "print loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with your **learning-rate** and **normalization** heuristic parameters we can go ahead and iteratively call the optimizer until the convergence. The number of iterations should be finite and we keep it 1000000$^{th}$ iteration or when the loss change is less than 1e-7 (or any smaller number). One can alway come back to this iteration and change the convergence factor; Note that we have used adaGradOptimizer, which is suppposed to be very robust optmizer and can be very liberal in accepting what learning-rate you want to give to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [float(\"Inf\")]\n",
    "epoch = 100000 #1000000\n",
    "#training loop\n",
    "for steps in  xrange(epoch):\n",
    "    losses.append(session.run([train, loss], feed_dict={x:x_data,y:y_data})[1])\n",
    "    #stop training if loss starts changing by less than 1e-7\n",
    "    if abs(losses[-1] - losses[-2]) < 10^-7:\n",
    "        break\n",
    "    if steps%10000 == 0:\n",
    "        print \"steps = %d  loss = %f\"%(steps,losses[steps])\n",
    "        \n",
    "print \"steps = %d loss = %f\"%(steps,losses[steps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot our resulting regresssion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref2.5\"></a>\n",
    "### Training the model \n",
    "\n",
    "Now we make one more call to $session.run$ which doesn't call the $\"loss\"$ function instead it just predicts the x for various values of $y$. Note that the optimization hasn't converged yet, and seems to have been still reducing at a slower rate. Given enough time one would definitely reach a much better solution than what we have got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get prediction with trained model\n",
    "pred,b0, b1, b2, b3 = session.run([y_pred, beta_0, beta_1, beta_2, beta_3], feed_dict={x:x_data,y:y_data})\n",
    "\n",
    "#plot the prediction against the datapoints\n",
    "plt.plot(x_data, pred)\n",
    "plt.plot(x_data, y_data, 'ro')\n",
    "\n",
    "#print the final parameters\n",
    "print\"beta_0 = %f, beta_1 = %f, beta_2 = %f, beta_3 = %f\" % (b0,b1, b2, b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below we can see how the loss function changes over the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The PowerAI platform supports popular machine learning libraries and dependencies including Tensorflow, Caffe, Torch, and Theano. You can download a [free version of PowerAI](https://cocl.us/ML0120EN_PAI).\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0120EN_DSX)This is the end of this lesson. Hopefully, now you have a deeper and intuitive understanding regarding the LSTM model. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by Jagadish Rangrej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html\n",
    "<br />\n",
    "https://en.wikipedia.org/wiki/Softmax_function\n",
    "<br />\n",
    "https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#classification\n",
    "\n",
    "Fernandez-Juricic, E., A. Sallent, R. Sanz, and I. Rodriguez-Prieto. 2003. Testing the risk-disturbance hypothesis in a fragmented landscape: non-linear responses of house sparrows to humans. Condor 105: 316-326."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

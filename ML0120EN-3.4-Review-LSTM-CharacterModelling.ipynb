{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big Data University](https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png)\n",
    "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>\n",
    "<br>    \n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "<br>\n",
    "- <p><a href=\"#arch\">Architectures</a></p>\n",
    "    - <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a>\n",
    "\n",
    "- <p><a href=\"#build\">Building a LSTM with TensorFlow</a></p>\n",
    "</div>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "#### Batch, number_of_batch, batch_size and seq_length\n",
    "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
    "\n",
    "Lets assume the input is 'here is an example'. Then:\n",
    "- txt_length = 18  \n",
    "- seq_length = 3  \n",
    "- batch_size = 2  \n",
    "- number_of_batch = 18/3*2 = 3\n",
    "- batch = array (['h','e','r'],['e',' ','i'])\n",
    "- sample Seq = 'her'  \n",
    "\n",
    "\n",
    "So, what are our actual parameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 60 #minibatch size, i.e. size of dataset in each epoch\n",
    "seq_length = 50 #RNN sequence length\n",
    "num_epochs = 25 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 #size of RNN hidden state\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture\n",
    "- each LSTM cell has an input layre, which its size is 128 units. \n",
    "- 128 is dimensionality of embedding vector.\n",
    "\n",
    "\n",
    "\n",
    "#### rnn_size = num_units = num_hidden_units:   = LSTM size\n",
    "\n",
    "\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units.\n",
    "- The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A).\n",
    "- Each LSTM cell keeps a vector, called __hidden state__ vector, of size n_hidden=128.\n",
    "- A __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \n",
    "- For each LSTM cell that we initialise, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "- \"num_units\" is equivalant to \"size of RNN hidden state\"\n",
    "- rnn_size= 128, is also the dimension size of W2V/embedding, for each character/word.\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - A __hidden state__ vector\n",
    "    - A __previous time-step output__\n",
    "- To make the name num_units more intuitive, you can think of it as the number of hidden units in the LSTM cell, or the number of memory units in the cell.\n",
    "- number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = TextLoader('/resources/data/', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = data_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =50, seq_length=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ..., \n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u' ', u'e', u't', u'o', u'a')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.chars[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.vocab['t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cell= rnn_cell.BasicLSTMCell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s).\n",
    "\n",
    "Args:\n",
    "\n",
    "batch_size: int, float, or unit Tensor representing the batch size.  \n",
    "dtype: the data type to use for the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict={input_data:x, targets:y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm',reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "        #input_data is a matrix of 60x50 and embedding is dictionary of 65x128 for all 65 characters\n",
    "        # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "        # it creates a 60*50*[1*128] matrix\n",
    "        # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "        em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "        # split: Splits a tensor into sub tensors.\n",
    "        # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "        # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "        inputs = tf.split(em, seq_length, 1)\n",
    "        # It will convert the list to 50 matrix of [60x128]\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01187611,  0.17317678, -0.04664408, ...,  0.13480385,\n",
       "        -0.17349039,  0.11645617],\n",
       "       [-0.09165053, -0.00296672,  0.08263053, ..., -0.10514037,\n",
       "         0.07863681,  0.13713406],\n",
       "       [ 0.15487878,  0.04418661, -0.07928608, ..., -0.01012498,\n",
       "         0.09112112,  0.02380088],\n",
       "       ..., \n",
       "       [-0.02088466, -0.1330204 ,  0.11572404, ...,  0.01860093,\n",
       "         0.061552  , -0.0512418 ],\n",
       "       [ 0.15729706, -0.17406626, -0.03938693, ..., -0.13803616,\n",
       "        -0.15875421, -0.00743723],\n",
       "       [-0.03634189, -0.10259962,  0.09615393, ...,  0.02035314,\n",
       "        -0.08798251, -0.01574321]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(60, 50, 128) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50, 128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "emp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08390545,  0.07798855,  0.04977153, ..., -0.01728435,\n",
       "         0.14431466, -0.11718105],\n",
       "       [-0.141221  , -0.06074303,  0.05824159, ..., -0.11142561,\n",
       "         0.11705144, -0.13420179],\n",
       "       [-0.10235118,  0.04645897, -0.09329474, ...,  0.04407293,\n",
       "        -0.15662232, -0.06366031],\n",
       "       ..., \n",
       "       [-0.09165053, -0.00296672,  0.08263053, ..., -0.10514037,\n",
       "         0.07863681,  0.13713406],\n",
       "       [ 0.04105198, -0.09031538,  0.08082359, ..., -0.08462127,\n",
       "         0.07042079, -0.07249702],\n",
       "       [-0.10235118,  0.04645897, -0.09329474, ...,  0.04407293,\n",
       "        -0.15662232, -0.06366031]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'split:0' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:1' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:2' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:3' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:4' shape=(60, 1, 128) dtype=float32>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is input in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08390545,  0.07798855,  0.04977153, ..., -0.01728435,\n",
       "         0.14431466, -0.11718105],\n",
       "       [-0.08402   ,  0.03561696,  0.00605483, ..., -0.15896314,\n",
       "         0.15770842,  0.16403507],\n",
       "       [-0.04141414, -0.09957065, -0.08418749, ..., -0.14920859,\n",
       "         0.09632279,  0.01807462],\n",
       "       ..., \n",
       "       [-0.05428317, -0.0974982 , -0.05203034, ...,  0.16225956,\n",
       "         0.08755989, -0.088445  ],\n",
       "       [-0.141221  , -0.06074303,  0.05824159, ..., -0.11142561,\n",
       "         0.11705144, -0.13420179],\n",
       "       [-0.16422763, -0.12701379,  0.14181305, ...,  0.10273062,\n",
       "         0.01999506,  0.11997597]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/multi_rnn_cell/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_1/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_2/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_3/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_4/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnnlm_1/multi_rnn_cell/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = outputs[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05821293, -0.05047727,  0.03240858, ..., -0.06629939,\n",
       "        -0.10825536,  0.01807678],\n",
       "       [ 0.00837175,  0.01522417, -0.02819392, ..., -0.01531586,\n",
       "         0.1428543 ,  0.0505182 ],\n",
       "       [ 0.00326554, -0.0340628 , -0.03086306, ...,  0.00487064,\n",
       "        -0.00767021,  0.05711473],\n",
       "       ..., \n",
       "       [-0.03196747, -0.22419493,  0.121751  , ..., -0.05133714,\n",
       "         0.07012291,  0.04810454],\n",
       "       [ 0.0887068 , -0.05727573, -0.07711516, ..., -0.00186371,\n",
       "         0.18001084,  0.06250025],\n",
       "       [-0.02076182, -0.06524993, -0.0664552 , ..., -0.01729499,\n",
       "         0.05801091,  0.02758405]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(test,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs is 50x[60*128]. We need to reshape it to [60x50x128]. Then we can calculate the softmax:\n",
    "\n",
    "softmax_w is [rnn_size, vocab_size], [128x65]\n",
    "\n",
    "[60x50x128]x[128x65]+[60x50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01452553,  0.01277425,  0.01863852, ...,  0.01512464,\n",
       "         0.01406453,  0.01335054],\n",
       "       [ 0.01180578,  0.01229866,  0.01665466, ...,  0.01562338,\n",
       "         0.01562235,  0.01671582],\n",
       "       [ 0.01471618,  0.01498685,  0.01340279, ...,  0.01755621,\n",
       "         0.01526808,  0.01378074],\n",
       "       ..., \n",
       "       [ 0.01179038,  0.0098834 ,  0.01629242, ...,  0.01548402,\n",
       "         0.01515435,  0.01294479],\n",
       "       [ 0.01756067,  0.01485729,  0.01396407, ...,  0.01787437,\n",
       "         0.01747026,  0.01640356],\n",
       "       [ 0.01610383,  0.01111977,  0.01510501, ...,  0.01382569,\n",
       "         0.0106961 ,  0.01195219]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits],\n",
    "                [tf.reshape(targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'div_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/multi_rnn_cell_49/cell_0/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_49/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = last_state\n",
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.ops.variables.Variable at 0x11f840050>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11f82bf50>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11f85bf50>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11f9f3190>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11f9f3050>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11fd68310>,\n",
       " <tensorflow.python.ops.variables.Variable at 0x11fd757d0>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'rnnlm/softmax_w:0',\n",
       " u'rnnlm/softmax_b:0',\n",
       " u'rnnlm/embedding:0',\n",
       " u'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/weights:0',\n",
       " u'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/biases:0',\n",
       " u'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/weights:0',\n",
       " u'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/biases:0',\n",
       " u'Variable:0']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(128, 65) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(65,) dtype=float32>,\n",
       " <tensorflow.python.framework.ops.IndexedSlices at 0x122071990>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(128,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(128,) dtype=float32>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.15889707e-04,   4.72889794e-03,   4.97568049e-04, ...,\n",
       "         -3.00469866e-04,  -2.99643085e-04,  -2.73773738e-04],\n",
       "       [ -9.02880263e-03,  -1.17456540e-03,  -5.33561828e-03, ...,\n",
       "          1.13346858e-03,   1.15507946e-03,   9.97719471e-04],\n",
       "       [ -3.42746335e-03,  -5.40494744e-04,  -1.60877389e-04, ...,\n",
       "          4.20372613e-04,   3.28794529e-04,   2.47172808e-04],\n",
       "       ..., \n",
       "       [ -7.71436235e-03,  -6.17193757e-03,   2.12614439e-04, ...,\n",
       "          7.91810919e-04,   7.32082524e-04,   6.88683067e-04],\n",
       "       [  9.13359970e-03,   5.39305434e-03,   2.35419301e-03, ...,\n",
       "         -9.34848678e-04,  -9.68575769e-04,  -9.20143095e-04],\n",
       "       [  2.37912731e-03,  -1.45130185e-03,  -6.81445003e-04, ...,\n",
       "         -5.27885277e-06,  -1.81617615e-05,  -3.04286186e-06]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using classes\n",
    "Now that we have learned how the networks work, we can put all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import seq2seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\"sample mode\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "        basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "the input is always a matrix of of shape [n x m]. Where n is the batch size, m is the feature size. \n",
    "In our case, the input shape will be [60 x ??]. \n",
    "\n",
    " \n",
    "size of data is 1113000, number of batches are 371, batch size is 60 and sequence length is 50. so, 50*60*371= 1113000\n",
    "\n",
    "we have 50 epochs. \n",
    "each input matrix will represent 1 update per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the LSTM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "e=1\n",
    "sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "data_loader.reset_batch_pointer()\n",
    "state = sess.run(model.initial_state)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "feed = {model.input_data: x, model.targets: y, model.initial_state:state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2406917"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.16106769,  0.09671967,  0.05100814, ..., -0.01635377,\n",
       "         -0.05336112,  0.05017365],\n",
       "        [ 0.03706374,  0.09157047,  0.16957512, ...,  0.35060707,\n",
       "          0.06505036,  0.16768083],\n",
       "        [-0.06861316,  0.35269612,  0.01392379, ..., -0.3016403 ,\n",
       "          0.15793857, -0.03690971],\n",
       "        ..., \n",
       "        [-0.09782275,  0.30612332,  0.01817104, ...,  0.12719224,\n",
       "         -0.07495581, -0.02366222],\n",
       "        [ 0.12180551,  0.20894924, -0.01887005, ...,  0.00422742,\n",
       "          0.11226939, -0.06858645],\n",
       "        [-0.09287884, -0.01052966,  0.20708023, ..., -0.0488694 ,\n",
       "         -0.0396448 , -0.02352033]], dtype=float32),\n",
       " array([[-0.14369003, -0.15190348, -0.10946867, ..., -0.19686896,\n",
       "         -0.26301304,  0.02058302],\n",
       "        [ 0.01852476, -0.09826656,  0.04434421, ...,  0.05310607,\n",
       "          0.07389302, -0.05907106],\n",
       "        [ 0.0751107 ,  0.22561377, -0.0044577 , ...,  0.28249529,\n",
       "         -0.25592875,  0.29575139],\n",
       "        ..., \n",
       "        [-0.07900319, -0.03534848,  0.23584716, ...,  0.13897093,\n",
       "          0.32787502, -0.07311685],\n",
       "        [-0.02276533,  0.01488482,  0.35613275, ..., -0.04029289,\n",
       "         -0.10554101,  0.13590965],\n",
       "        [ 0.36544853, -0.33561194, -0.08476857, ..., -0.13857377,\n",
       "          0.01031023,  0.34008068]], dtype=float32))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train usinng LSTMModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370/9275 (epoch 0), train_loss = 1.875, time/batch = 0.043\n",
      "741/9275 (epoch 1), train_loss = 1.708, time/batch = 0.041\n",
      "1112/9275 (epoch 2), train_loss = 1.633, time/batch = 0.054\n",
      "1483/9275 (epoch 3), train_loss = 1.593, time/batch = 0.046\n",
      "1854/9275 (epoch 4), train_loss = 1.567, time/batch = 0.053\n",
      "2225/9275 (epoch 5), train_loss = 1.548, time/batch = 0.041\n",
      "2596/9275 (epoch 6), train_loss = 1.535, time/batch = 0.039\n",
      "2967/9275 (epoch 7), train_loss = 1.525, time/batch = 0.049\n",
      "3338/9275 (epoch 8), train_loss = 1.517, time/batch = 0.040\n",
      "3709/9275 (epoch 9), train_loss = 1.509, time/batch = 0.043\n",
      "4080/9275 (epoch 10), train_loss = 1.502, time/batch = 0.047\n",
      "4451/9275 (epoch 11), train_loss = 1.494, time/batch = 0.040\n",
      "4822/9275 (epoch 12), train_loss = 1.487, time/batch = 0.036\n",
      "5193/9275 (epoch 13), train_loss = 1.481, time/batch = 0.039\n",
      "5564/9275 (epoch 14), train_loss = 1.477, time/batch = 0.038\n",
      "5935/9275 (epoch 15), train_loss = 1.473, time/batch = 0.036\n",
      "6306/9275 (epoch 16), train_loss = 1.470, time/batch = 0.036\n",
      "6677/9275 (epoch 17), train_loss = 1.467, time/batch = 0.035\n",
      "7048/9275 (epoch 18), train_loss = 1.464, time/batch = 0.037\n",
      "7419/9275 (epoch 19), train_loss = 1.460, time/batch = 0.035\n",
      "7790/9275 (epoch 20), train_loss = 1.457, time/batch = 0.037\n",
      "8161/9275 (epoch 21), train_loss = 1.453, time/batch = 0.035\n",
      "8532/9275 (epoch 22), train_loss = 1.450, time/batch = 0.038\n",
      "8903/9275 (epoch 23), train_loss = 1.446, time/batch = 0.037\n",
      "9274/9275 (epoch 24), train_loss = 1.443, time/batch = 0.035\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        #model.sample(sess, data_loader.chars , data_loader.vocab, num=200, prime='The ', sampling_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mode\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope(\"sample_test\"):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    m = LSTMModel(sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prime='The '\n",
    "num=200\n",
    "sampling_type=1\n",
    "vocab=data_loader.vocab\n",
    "chars=data_loader.chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(m.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print state\n",
    "sess.run(tf.global_variables_initializer())\n",
    "state=sess.run(m.initial_state)\n",
    "for char in prime[:-1]:\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {m.input_data: x, m.initial_state:state}\n",
    "    [state] = sess.run([m.final_state], feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05878098,  0.05151086, -0.01658276, -0.01361227, -0.07602809,\n",
       "          0.09598655, -0.15346964, -0.29651856, -0.14367703, -0.03699737,\n",
       "         -0.13778403,  0.0206705 , -0.0425937 ,  0.06101717,  0.01471224,\n",
       "          0.07944712, -0.00359382,  0.07064421, -0.05881016,  0.02421402,\n",
       "          0.27702171,  0.09948913,  0.01396049,  0.24405159, -0.15082668,\n",
       "          0.09722668,  0.04478887, -0.13433081, -0.18657054, -0.0896575 ,\n",
       "          0.02983936,  0.14230509, -0.08031639, -0.07138512, -0.01722506,\n",
       "         -0.07718454, -0.06950866, -0.16460122,  0.0498888 , -0.01911711,\n",
       "         -0.00944835,  0.03433346, -0.19992422, -0.12076587,  0.07408935,\n",
       "         -0.01468888,  0.32369787, -0.09865332,  0.19121619,  0.09646589,\n",
       "          0.14042749,  0.11068583, -0.00829003,  0.02230815, -0.08730035,\n",
       "         -0.13893236, -0.1292657 , -0.18696299,  0.14410619,  0.1721911 ,\n",
       "         -0.06488436,  0.17704102,  0.13975242, -0.16831143,  0.04572584,\n",
       "         -0.07830398, -0.14049169,  0.02179072, -0.10536504, -0.06877302,\n",
       "         -0.08299193,  0.1016828 ,  0.0530383 ,  0.05381666, -0.16837749,\n",
       "          0.040799  , -0.07598498, -0.2700018 ,  0.06280172,  0.03953955,\n",
       "         -0.07047758,  0.17343462, -0.04744704, -0.06361077,  0.048208  ,\n",
       "         -0.18626514, -0.02428605, -0.16712035, -0.00430273,  0.08030148,\n",
       "         -0.04681473, -0.07094866, -0.06587552, -0.10440148,  0.18764111,\n",
       "          0.05080188,  0.08820473,  0.0873989 ,  0.21854568,  0.00224408,\n",
       "          0.09936513,  0.05541606, -0.10306382,  0.13431185,  0.02527339,\n",
       "         -0.01139284, -0.05383827, -0.10692737, -0.07865649,  0.12472347,\n",
       "          0.11436322,  0.1225279 ,  0.0575162 , -0.03871185,  0.06491324,\n",
       "          0.17142202, -0.07384697,  0.10066696, -0.00648126,  0.12426291,\n",
       "         -0.05007403, -0.31792402, -0.03680744,  0.0832444 ,  0.0099714 ,\n",
       "         -0.11400834,  0.16446503,  0.10789781]], dtype=float32),\n",
       " array([[ 0.17890573,  0.03159131, -0.02232169, -0.13083938,  0.10068867,\n",
       "          0.16506922,  0.0923366 , -0.05379436, -0.15684006, -0.01237823,\n",
       "          0.04204702, -0.13445318, -0.02499081, -0.01654569,  0.03458223,\n",
       "         -0.23809692, -0.08209138, -0.06642897, -0.18797004, -0.01658112,\n",
       "         -0.04964022,  0.03409923,  0.05533918,  0.1221004 , -0.01021517,\n",
       "          0.01231153, -0.10734623,  0.02540391,  0.0154467 , -0.02563982,\n",
       "         -0.03043636,  0.00230107,  0.09550625, -0.08168003, -0.22255185,\n",
       "          0.09637885, -0.07368282,  0.07764421,  0.3358939 , -0.01805203,\n",
       "          0.23115641,  0.13071279, -0.01906963,  0.12275877,  0.22359408,\n",
       "          0.02960084,  0.07213347, -0.03956942, -0.01138084,  0.0871662 ,\n",
       "         -0.09086813,  0.12415729,  0.12403004,  0.0788874 , -0.13195045,\n",
       "          0.15232763, -0.1313023 , -0.00201749, -0.01112306, -0.08011279,\n",
       "         -0.22216213, -0.09239633,  0.01282636,  0.01109545,  0.0306908 ,\n",
       "          0.10793082,  0.05594183, -0.04202056, -0.09693771, -0.21857129,\n",
       "         -0.00045276,  0.2009467 ,  0.05269178,  0.18297094, -0.091347  ,\n",
       "         -0.30214724, -0.0588924 , -0.09873115,  0.05367579,  0.21271832,\n",
       "          0.1163554 ,  0.1210951 ,  0.12625003, -0.0970434 ,  0.04673495,\n",
       "          0.07503369, -0.01397476, -0.06079275,  0.08386295,  0.06948304,\n",
       "          0.20066717, -0.07126471, -0.10018766, -0.07427699, -0.01764298,\n",
       "         -0.04820573,  0.02099933,  0.23293555,  0.07338541, -0.06869493,\n",
       "         -0.03630202, -0.07053632,  0.03961857,  0.19435091,  0.06001865,\n",
       "         -0.00543278, -0.09310275,  0.15167904,  0.15458927,  0.04062539,\n",
       "         -0.04853304,  0.30574766, -0.15071613,  0.19952598, -0.00903228,\n",
       "         -0.01314319, -0.04286902, -0.14244258,  0.07445141, -0.11117797,\n",
       "         -0.02591677,  0.11507215, -0.10862765, -0.05164803, -0.09273403,\n",
       "          0.16559854, -0.17970914, -0.11195239]], dtype=float32))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "ret = prime\n",
    "char = prime[-1]\n",
    "for n in range(num):\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {m.input_data: x, m.initial_state:state}\n",
    "    [probs, state] = sess.run([m.probs, m.final_state], feed)\n",
    "    p = probs[0]\n",
    "\n",
    "    if sampling_type == 0:\n",
    "        sample = np.argmax(p)\n",
    "    elif sampling_type == 2:\n",
    "        if char == ' ':\n",
    "            sample = weighted_pick(p)\n",
    "        else:\n",
    "            sample = np.argmax(p)\n",
    "    else: # sampling_type == 1 default:\n",
    "        sample = weighted_pick(p)\n",
    "\n",
    "    pred = chars[sample]\n",
    "    ret += pred\n",
    "    char = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"The grWax'WNnm$oO;3KPcu!REmeIIcbN.DDUzp\\nlNvF!NyKJgAr'qeSKgR'h,;WHOkBpKdGJjGX'x?IKJLQ!JEkzpqCwM-\\n'hZCxF;rSVNEdXelGltCJOlj:Gua:pvyr&oQ\\nkVvMh''D?B$AgxEX!i:P:w.cz'JV!U'TNKvYwTizAgJ?fUgZkSI$kJnp3'lWJi$&ZIARViD\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32), array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u\"The  B&;x-kHZTs;suVlcjvgAe-M!EnlK$EaeseKcRCsTGWEC..M?Gudy$lIu'&KbvUOE!BJ&HGb,:CdJcpSdiV'ifX..:rdl.Dio:qIMxgsdXotdxJV i3'BekMuQ;BQdWqmdgW$f&Crw'UKMYGag:Hegq l$?Twh$wmHsbEP,kssVksshzAxY$ggHQe$dOxrwQHHP\\n3g$v\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "state=sess.run(m.initial_state)\n",
    "m.sample(sess, data_loader.chars , data_loader.vocab, num=200, prime='The ', sampling_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
